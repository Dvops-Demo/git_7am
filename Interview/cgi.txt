C1,C2,C3,C4,C5
1001,Satya,CGI,Hyderabad,2000
1001,Satya,CGI,Bangalore,4000
1001,Satya,CGI,Chennai,6000
1002,Sathish,CGI,Hyderabad,10000
1002,sathish,CGI,Bangalore,40000
1002,Sathish,CGI,Chennai,20000

>>> spark.sql("select *,sum(C5) over(order by c1) as sum_sal from empt").show()
23/11/16 10:47:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
+----+-----+---+---------+----+-------+
|  C1|   C2| C3|       C4|  C5|sum_sal|
+----+-----+---+---------+----+-------+
|1001|Satya|CGI|Hyderabad|2000|  12000|
|1001|Satya|CGI|Bangalore|4000|  12000|
|1001|Satya|CGI|  Chennai|6000|  12000|
+----+-----+---+---------+----+-------+

>>> spark.sql("select *,avg(C5) over(order by c1) as avg_sal from empt").show()
23/11/16 10:48:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
+----+-----+---+---------+----+-------+
|  C1|   C2| C3|       C4|  C5|avg_sal|
+----+-----+---+---------+----+-------+
|1001|Satya|CGI|Hyderabad|2000| 4000.0|
|1001|Satya|CGI|Bangalore|4000| 4000.0|
|1001|Satya|CGI|  Chennai|6000| 4000.0|
+----+-----+---+---------+----+-------+

>>> spark.sql("select *,min(C5) over(order by c1) as min_sal from empt").show()
23/11/16 10:49:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
+----+-----+---+---------+----+-------+
|  C1|   C2| C3|       C4|  C5|min_sal|
+----+-----+---+---------+----+-------+
|1001|Satya|CGI|Hyderabad|2000|   2000|
|1001|Satya|CGI|Bangalore|4000|   2000|
|1001|Satya|CGI|  Chennai|6000|   2000|
+----+-----+---+---------+----+-------+

>>> spark.sql("select *,max(C5) over(order by c1) as max_sal from empt").show()
23/11/16 10:50:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
+----+-----+---+---------+----+-------+
|  C1|   C2| C3|       C4|  C5|max_sal|
+----+-----+---+---------+----+-------+
|1001|Satya|CGI|Hyderabad|2000|   6000|
|1001|Satya|CGI|Bangalore|4000|   6000|
|1001|Satya|CGI|  Chennai|6000|   6000|
+----+-----+---+---------+----+-------+

>>> spark.sql("select * from T1").show(30)
+----+
|  C1|
+----+
|   1|
|   1|
|   1|
|   1|
|   1|
|   1|
|   1|
|   2|
|   2|
|NULL|
|NULL|
+----+

>>> spark.sql("select * from T2").show(30)
+----+
|  C2|
+----+
|   1|
|   1|
|   1|
|   3|
|NULL|
+----+



>>> spark.sql("select * from T1 full join T2 on T1.C1=T2.C2").show(30)
+----+----+
|  C1|  C2|
+----+----+
|null|   3|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   2|null|
|   2|null|
|NULL|NULL|
|NULL|NULL|
+----+----+

>>> spark.sql("select * from T1 left join T2 on T1.C1=T2.C2").show(30)
+----+----+
|  C1|  C2|
+----+----+
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   2|null|
|   2|null|
|NULL|NULL|
|NULL|NULL|
+----+----+

>>> spark.sql("select * from T1 inner join T2 on T1.C1=T2.C2").show(30)
+----+----+
|  C1|  C2|
+----+----+
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|NULL|NULL|
|NULL|NULL|
+----+----+

>>> spark.sql("select * from T1 right join T2 on T1.C1=T2.C2").show(30)
+----+----+
|  C1|  C2|
+----+----+
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|   1|   1|
|null|   3|
|NULL|NULL|
|NULL|NULL|
+----+----+

>>> mul_df1=spark.read.options(header="true",inferSchema="true").csv("fm1.csv")
>>> mul_df2=spark.read.options(header="true",inferSchema="true").csv("fm2.csv")
>>> mul_df3=spark.read.options(header="true",inferSchema="true").csv("fm3.csv")
>>> mul_df1.union(mul_df2)
DataFrame[a: int, b: int, c: int]
>>> df12=mul_df1.union(mul_df2)
>>> df12.show()
+---+---+---+
|  a|  b|  c|
+---+---+---+
|  1|  1|  1|
|  2|  2|  2|
+---+---+---+

>>> df123=df12.union(mul_df3)
>>> df123.show()
+---+---+---+
|  a|  b|  c|
+---+---+---+
|  1|  1|  1|
|  2|  2|  2|
|  3|  3|  3|
+---+---+---+
